{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import myFunction as mf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "from sklearn.externals import joblib\n",
    "#import joblib\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import stats\n",
    "import myFunction as mf\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None) #show all columns\n",
    "pd.set_option('max_colwidth',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'result/trainAndTestV6.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c8bba3c42c6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'result/trainAndTestV6.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0msplit_percentage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0msub_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0moutcome_variables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'result/trainAndTestV6.csv'"
     ]
    }
   ],
   "source": [
    "#train test data split\n",
    "train_variables=['gender', 'age', 'vasopressin',  'urineoutput', 'heartrate_mean', \n",
    "                 'sysbp_mean', 'diasbp_mean', 'resprate_mean', 'tempc_mean', 'spo2_mean',\n",
    "                 'baseexcess_mean', 'totalco2_mean', 'calcium_mean', 'lactate_mean', \n",
    "       'pco2_mean', 'ph_mean', 'po2_mean',\n",
    "       'coronary heart disease.csv', 'diabetes.csv', 'family history of stroke.csv', \n",
    "       'creatinine.csv_mean',\n",
    "       'glucose.csv_mean', 'platelet.csv_mean', 'potassium.csv_mean',\n",
    "       'sodium.csv_mean', 'urea nitrogen.csv_mean', 'WBC.csv_mean', 'aniongap',\n",
    "       'bicarbonate', 'hematocrit', 'hemoglobin', 'ptt', 'inr', 'pt', 'BMI']\n",
    "outcome_variables=['liver_dysfunction','thrombocytopenia','hospital_mortality',\n",
    "                  'septic_shock']\n",
    "\n",
    "\n",
    "data=pd.read_csv('result/trainAndTestV6.csv')\n",
    "split_percentage=0.8\n",
    "sub_data=data[list(train_variables)+outcome_variables]\n",
    "train=sub_data[:int(len(data)*split_percentage)]\n",
    "test=sub_data[int(len(data)*split_percentage):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "report=[]\n",
    "count=0\n",
    "list2=[]\n",
    "for i in range(500,9001,100):\n",
    "    list2.append(i)\n",
    "list2\n",
    "for time,scale_pos_weight in enumerate([0.01,0.02,0.03,0.04,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80\n",
    "                                        ,90,100,200,300,400,500]+list2):\n",
    "    for i in outcome_variables:\n",
    "        count=count+1\n",
    "        parameters=[{'n_estimators': [30,60], \n",
    "                'max_depth': [30,60],\n",
    "                'reg_lambda':[0.1,1,10],\n",
    "                'gamma':[0.1,1,10],\n",
    "                'scale_pos_weight':[scale_pos_weight]}]\n",
    "        model=GridSearchCV(XGBClassifier(n_jobs=-1), parameters, cv=5)\n",
    "        model.fit(train[train_variables], train[i])\n",
    "        y_score=model.predict_proba(test[train_variables])[:,1]\n",
    "        fpr, tpr, _ = roc_curve(test[i], y_score)\n",
    "        AUROC=auc(fpr, tpr)\n",
    "        trainS=model.score(train[train_variables], train[i])\n",
    "        testS=model.score(test[train_variables], test[i])\n",
    "        f1S=metrics.f1_score(test[i], model.predict(test[train_variables]))\n",
    "        preS=metrics.precision_score(test[i], model.predict(test[train_variables]))\n",
    "        recS=metrics.recall_score(test[i], model.predict(test[train_variables]))\n",
    "        AUROC=auc(fpr, tpr)\n",
    "        feature_importance=pd.DataFrame({'column':train_variables, 'values':model.best_estimator_.feature_importances_})\n",
    "        feature_importance.sort_values(by='values', axis=0, ascending=False, inplace=True)\n",
    "        report.append([trainS, testS, f1S, preS,recS,AUROC,model.best_params_, i,\n",
    "                      feature_importance.iloc[0].values,feature_importance.iloc[1].values,feature_importance.iloc[2].values,\n",
    "                      feature_importance.iloc[3].values,feature_importance.iloc[4].values,scale_pos_weight])\n",
    "        print(i,':')\n",
    "        print('scale:', scale_pos_weight)\n",
    "        print(trainS,'-',testS,'-',f1S,'-',preS,'-',recS,'-',AUROC,'-',model.best_params_)\n",
    "        print('---------------------------------')\n",
    "    res=pd.DataFrame(report)\n",
    "    res.to_csv('result/trained models/XGB_models/report.csv')\n",
    "    #joblib.dump(model, 'result/trained models/XGB_models/'+i+'_XGB.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "\n",
    "\n",
    "report=[]\n",
    "count=0\n",
    "for time,scale_pos_weight in enumerate([0.01,0.02,0.03,0.04,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80\n",
    "                                        ,90,100,200,300,400,500]+list2):\n",
    "    for j, i in enumerate(outcome_variables):\n",
    "        count=count+1\n",
    "        parameters=[{'C':[0.01,0.1,1,10],\n",
    "                    'class_weight':['balanced'],\n",
    "                    'max_iter':[5]}]\n",
    "        model=GridSearchCV(LogisticRegression(n_jobs=-1), parameters, cv=5)\n",
    "        model.fit(train[train_variables], train[i])\n",
    "        y_score=model.predict_proba(test[train_variables])[:,1]\n",
    "        fpr, tpr, _ = roc_curve(test[i], y_score)\n",
    "        AUROC=auc(fpr, tpr)\n",
    "        trainS=model.score(train[train_variables], train[i])\n",
    "        testS=model.score(test[train_variables], test[i])\n",
    "        f1S=metrics.f1_score(test[i], model.predict(test[train_variables]))\n",
    "        preS=metrics.precision_score(test[i], model.predict(test[train_variables]))\n",
    "        recS=metrics.recall_score(test[i], model.predict(test[train_variables]))\n",
    "        AUROC=auc(fpr, tpr)\n",
    "        #feature_importance=pd.DataFrame({'column':train_variables, 'values':model.best_estimator_.coef_})\n",
    "        #feature_importance.sort_values(by='values', axis=0, ascending=False, inplace=True)\n",
    "        report.append([trainS, testS, f1S, preS,recS,AUROC,model.best_params_, i])\n",
    "        #              feature_importance.iloc[0].values,feature_importance.iloc[1].values,feature_importance.iloc[2].values,\n",
    "        #              feature_importance.iloc[3].values,feature_importance.iloc[4].values])\n",
    "        print(i,':')\n",
    "        print(trainS,'-',testS,'-',f1S,'-',preS,'-',recS,'-',AUROC,'-',model.best_params_)\n",
    "        print('---------------------------------')\n",
    "        res=pd.DataFrame(report)\n",
    "        #res.to_csv('result/trained models/LR_models/report.csv')\n",
    "        #joblib.dump(model, 'result/trained models/LR_models/'+i+'_LR.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "\n",
    "report=[]\n",
    "count=0\n",
    "for time,scale_pos_weight in enumerate([0.01,0.02,0.03,0.04,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80\n",
    "                                        ,90,100,200,300,400,500]+list2):\n",
    "    for i in outcome_variables:\n",
    "        count=count+1\n",
    "        parameters=[{'n_estimators':[30,60],\n",
    "                    'max_depth':[30,60],\n",
    "                    'max_leaf_nodes':[30,60],\n",
    "                    'class_weight':[scale_pos_weight]}]\n",
    "        model=GridSearchCV(RandomForestClassifier(), parameters, cv=5)\n",
    "        model.fit(train[train_variables], train[i])\n",
    "        y_score=model.predict_proba(test[train_variables])[:,1]\n",
    "        fpr, tpr, _ = roc_curve(test[i], y_score)\n",
    "        AUROC=auc(fpr, tpr)\n",
    "        trainS=model.score(train[train_variables], train[i])\n",
    "        testS=model.score(test[train_variables], test[i])\n",
    "        f1S=metrics.f1_score(test[i], model.predict(test[train_variables]))\n",
    "        preS=metrics.precision_score(test[i], model.predict(test[train_variables]))\n",
    "        recS=metrics.recall_score(test[i], model.predict(test[train_variables]))\n",
    "        AUROC=auc(fpr, tpr)\n",
    "        #feature_importance=pd.DataFrame({'column':train_variables, 'values':model.best_estimator_.coef_})\n",
    "        #feature_importance.sort_values(by='values', axis=0, ascending=False, inplace=True)\n",
    "        report.append([trainS, testS, f1S, preS,recS,AUROC,model.best_params_, i])\n",
    "        #              feature_importance.iloc[0].values,feature_importance.iloc[1].values,feature_importance.iloc[2].values,\n",
    "        #              feature_importance.iloc[3].values,feature_importance.iloc[4].values])\n",
    "        #print(i,':')\n",
    "        #print(trainS,'-',testS,'-',f1S,'-',preS,'-',recS,'-',AUROC,'-',model.best_params_)\n",
    "        #print('---------------------------------')\n",
    "    res=pd.DataFrame(report)\n",
    "    #res.to_csv('result/trained models/RF_models/RF_report.csv')\n",
    "        #joblib.dump(model, 'result/trained models/RF_models/'+i+'_RF.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#artificial neural network\n",
    "\n",
    "report=[]\n",
    "count=0\n",
    "for i in outcome_variables:\n",
    "    count=count+1\n",
    "    parameters=[{'hidden_layer_sizes':[(50,50),(100,100),(150,150)]}]\n",
    "    #model=GridSearchCV(RandomForestClassifier(), parameters, cv=5)\n",
    "    model=GridSearchCV(MLPClassifier(), parameters, cv=5)\n",
    "    #model=RandomForestClassifier(class_weight='balanced')\n",
    "    model.fit(train[train_variables], train[i])\n",
    "\n",
    "    y_score=model.predict_proba(test[train_variables])[:,1]\n",
    "    fpr, tpr, _ = roc_curve(test[i], y_score)\n",
    "    AUROC=auc(fpr, tpr)\n",
    "    trainS=model.score(train[train_variables], train[i])\n",
    "    testS=model.score(test[train_variables], test[i])\n",
    "    f1S=metrics.f1_score(test[i], model.predict(test[train_variables]))\n",
    "    preS=metrics.precision_score(test[i], model.predict(test[train_variables]))\n",
    "    recS=metrics.recall_score(test[i], model.predict(test[train_variables]))\n",
    "    AUROC=auc(fpr, tpr)\n",
    "    #feature_importance=pd.DataFrame({'column':train_variables, 'values':model.best_estimator_.coef_})\n",
    "    #feature_importance.sort_values(by='values', axis=0, ascending=False, inplace=True)\n",
    "    report.append([trainS, testS, f1S, preS,recS,AUROC,model.best_params_, i])\n",
    "    #              feature_importance.iloc[0].values,feature_importance.iloc[1].values,feature_importance.iloc[2].values,\n",
    "    #              feature_importance.iloc[3].values,feature_importance.iloc[4].values])\n",
    "    #print(i,':')\n",
    "    #print(trainS,'-',testS,'-',f1S,'-',preS,'-',recS,'-',AUROC,'-',model.best_params_)\n",
    "    #print('---------------------------------')\n",
    "    res=pd.DataFrame(report)\n",
    "    res.to_csv('result/trained models/ANN_models/report.csv')\n",
    "    joblib.dump(model, 'result/trained models/ANN_models/'+i+'_ANN.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve\n",
    "\n",
    "name=['AKF','CKD','hospital mortality','inflammatory','pneumonia','sepsis','septic shock','Thrombocytopenia']\n",
    "test_name=['AKF.csv','CKD.csv','hospital_mortality', 'inflammatory reaction.csv','pneumonia.csv',\n",
    "         'sepsis.csv','septic shock.csv','Thrombocytopenia.csv']\n",
    "\n",
    "for i in range(len(name)):\n",
    "    plt.figure(dpi=500)\n",
    "    modelXGB=joblib.load('result/trained models/XGB_models/'+test_name[i]+'_XGB.model')\n",
    "    y_score=modelXGB.predict_proba(test[train_variables])[:,1]\n",
    "    fpr, tpr, _ = roc_curve(test[test_name[i]], y_score)\n",
    "    AUROC=auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color='blue', label='XGB (AUROC=%0.2f)'%AUROC)\n",
    "    modelANN=joblib.load('result/trained models/ANN_models/'+test_name[i]+'_ANN.model')\n",
    "    y_score=modelANN.predict_proba(test[train_variables])[:,1]\n",
    "    fpr, tpr, _ = roc_curve(test[test_name[i]], y_score)\n",
    "    AUROC=auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color='darkorange', label='ANN (AUROC=%0.2f)'%AUROC)\n",
    "    modelLR=joblib.load('result/trained models/LR_models/'+test_name[i]+'_LR.model')\n",
    "    y_score=modelLR.predict_proba(test[train_variables])[:,1]\n",
    "    fpr, tpr, _ = roc_curve(test[test_name[i]], y_score)\n",
    "    AUROC=auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color='red', label='LR (AUROC=%0.2f)'%AUROC)\n",
    "    modelRF=joblib.load('result/trained models/RF_models/'+test_name[i]+'_RF.model')\n",
    "    y_score=modelRF.predict_proba(test[train_variables])[:,1]\n",
    "    fpr, tpr, _ = roc_curve(test[test_name[i]], y_score)\n",
    "    AUROC=auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color='green', label='RF (AUROC=%0.2f)'%AUROC)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.title('ROC curve of '+name[i]+' model')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('result/ROC curve/'+name[i]+'.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score table\n",
    "\n",
    "model_name=['XGB','ANN','LR','RF']\n",
    "res=[]\n",
    "for i in range(len(test_name)):\n",
    "    modelXGB=joblib.load('result/trained models/XGB_models/'+test_name[i]+'_XGB.model')\n",
    "    modelANN=joblib.load('result/trained models/ANN_models/'+test_name[i]+'_ANN.model')\n",
    "    modelLR=joblib.load('result/trained models/LR_models/'+test_name[i]+'_LR.model')\n",
    "    modelRF=joblib.load('result/trained models/RF_models/'+test_name[i]+'_RF.model')\n",
    "    model_list=[modelXGB,modelANN,modelLR,modelRF]\n",
    "    for j in range(len(model_list)):\n",
    "        y_score=model_list[j].predict_proba(test[train_variables])[:,1]\n",
    "        fpr, tpr, _ = roc_curve(test[test_name[i]], y_score)\n",
    "        trainS=model_list[j].score(train[train_variables], train[test_name[i]])\n",
    "        testS=model_list[j].score(test[train_variables], test[test_name[i]])\n",
    "        f1S=metrics.f1_score(test[test_name[i]], model_list[j].predict(test[train_variables]))\n",
    "        preS=metrics.precision_score(test[test_name[i]], model_list[j].predict(test[train_variables]))\n",
    "        recS=metrics.recall_score(test[test_name[i]], model_list[j].predict(test[train_variables]))\n",
    "        AUROC=auc(fpr, tpr)\n",
    "        #print(trainS,'-',testS,'-',f1S,'-',preS,'-',recS,'-',AUROC)\n",
    "        #print(metrics.confusion_matrix(test[i], model.predict(test[train_variables])))\n",
    "        res.append([trainS,testS,f1S,preS,recS,AUROC,name[i],model_name[j]])  \n",
    "res=pd.DataFrame(res)\n",
    "res.to_csv('result/ROC curve/score table.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "report=[]\n",
    "count=0\n",
    "\n",
    "for time,scale_pos_weight in enumerate([30]):\n",
    "    for i in outcome_variables:\n",
    "        count=count+1\n",
    "        parameters=[{'n_estimators': [60], \n",
    "                    'max_depth': [30],\n",
    "                    'reg_lambda':[10],\n",
    "                    'gamma':[0.1],\n",
    "                    'scale_pos_weight':[scale_pos_weight]}]\n",
    "        model=GridSearchCV(XGBClassifier(n_jobs=-1), parameters, cv=5)\n",
    "        model.fit(train[train_variables], train[i])\n",
    "        y_score=model.predict_proba(test[train_variables])[:,1]\n",
    "        fpr, tpr, _ = roc_curve(test[i], y_score)\n",
    "        AUROC=auc(fpr, tpr)\n",
    "        trainS=model.score(train[train_variables], train[i])\n",
    "        testS=model.score(test[train_variables], test[i])\n",
    "        f1S=metrics.f1_score(test[i], model.predict(test[train_variables]))\n",
    "        preS=metrics.precision_score(test[i], model.predict(test[train_variables]))\n",
    "        recS=metrics.recall_score(test[i], model.predict(test[train_variables]))\n",
    "        AUROC=auc(fpr, tpr)\n",
    "        feature_importance=pd.DataFrame({'column':train_variables, 'values':model.best_estimator_.feature_importances_})\n",
    "        feature_importance.sort_values(by='values', axis=0, ascending=False, inplace=True)\n",
    "        report.append([trainS, testS, f1S, preS,recS,AUROC,model.best_params_, i,\n",
    "                      feature_importance.iloc[0].values,feature_importance.iloc[1].values,feature_importance.iloc[2].values,\n",
    "                      feature_importance.iloc[3].values,feature_importance.iloc[4].values,scale_pos_weight])\n",
    "        print(i,':')\n",
    "        print('scale:', scale_pos_weight)\n",
    "        print(trainS,'-',testS,'-',f1S,'-',preS,'-',recS,'-',AUROC,'-',model.best_params_)\n",
    "        print('---------------------------------')\n",
    "    res=pd.DataFrame(report)\n",
    "    #res.to_csv('result/trained models/XGB_models/report.csv')\n",
    "    joblib.dump(model, 'result/trained models/XGB_models/'+i+'_XGB.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=['test/hospital_mortality_XGB.model','test/liver_dysfunction_XGB.model',\n",
    "          'test/septic_shock_XGB.model','test/thrombocytopenia_XGB.model']\n",
    "y_name=['hospital_mortality','liver_dysfunction','septic_shock','thrombocytopenia']\n",
    "\n",
    "for i in range(4):\n",
    "    model=joblib.load(file_name[i])\n",
    "    y_score=model.predict_proba(test[train_variables])[:,1]\n",
    "    fpr, tpr, _ = roc_curve(test[y_name[i]], y_score)\n",
    "    AUROC=auc(fpr, tpr)\n",
    "    trainS=model.score(train[train_variables], train[y_name[i]])\n",
    "    testS=model.score(test[train_variables], test[y_name[i]])\n",
    "    f1S=metrics.f1_score(test[y_name[i]], model.predict(test[train_variables]))\n",
    "    preS=metrics.precision_score(test[y_name[i]], model.predict(test[train_variables]))\n",
    "    recS=metrics.recall_score(test[y_name[i]], model.predict(test[train_variables]))\n",
    "    AUROC=auc(fpr, tpr)\n",
    "    print(y_name[i],':',trainS,'-',testS,'-',f1S,'-',preS,'-',recS,'-',AUROC,'-',model.best_params_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=joblib.load('result/trained models/XGB_models/hospital_mortality_XGB.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance=pd.DataFrame({'column':train_variables, 'values':model.best_estimator_.feature_importances_})\n",
    "feature_importance.sort_values(by='values', axis=0, ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
